
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Blur2Blur</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://jonbarron.info/zipnerf/img/nottingham.jpg">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1296">
    <meta property="og:image:height" content="840">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="zero1778.github.io/blur2blur"/>
    <meta property="og:title" content="Blur2Blur: Blur Conversion for Unsupervised Image Deblurring on Unknown Domains" />
    <meta property="og:description" content="This paper presents an innovative framework designed to train an image deblurring algorithm tailored to a specific camera device. This algorithm works by transforming a blurry input image, which is challenging to deblur, into another blurry image that is more amenable to deblurring. The transformation process, from one blurry state to another, leverages unpaired data consisting of sharp and blurry images captured by the target camera device. Learning this blur-to-blur transformation is inherently simpler than direct blur-to-sharp conversion, as it primarily involves modifying blur patterns rather than the intricate task of reconstructing fine image details. The efficacy of the proposed approach has been demonstrated through comprehensive experiments on various benchmarks, where it significantly outperforms state-of-the-art methods both quantitatively and qualitatively." />

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Blur2Blur: Blur Conversion for Unsupervised Image Deblurring on Unknown Domains" />
    <meta name="twitter:description" content="This paper presents an innovative framework designed to train an image deblurring algorithm tailored to a specific camera device. This algorithm works by transforming a blurry input image, which is challenging to deblur, into another blurry image that is more amenable to deblurring. The transformation process, from one blurry state to another, leverages unpaired data consisting of sharp and blurry images captured by the target camera device. Learning this blur-to-blur transformation is inherently simpler than direct blur-to-sharp conversion, as it primarily involves modifying blur patterns rather than the intricate task of reconstructing fine image details. The efficacy of the proposed approach has been demonstrated through comprehensive experiments on various benchmarks, where it significantly outperforms state-of-the-art methods both quantitatively and qualitatively." />
    <meta name="twitter:image" content="zero1778.github.io/blur2blur/img/Teaser.pdf" />


<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>âš¡</text></svg>">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
    <script src="js/video_comparison.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <!-- Blur2Blur: Blur Conversion for Unsupervised Image Deblurring on Unknown Domains -->
                <b>Blur2Blur</b>: Blur Conversion for Unsupervised Image Deblurring <br>on Unknown Domains</br> 
                
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="zero1778.github.io">
                          Bang-Dang Pham<sup>1</sup>
                        </a>
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?hl=en&authuser=1&user=-BPaFHcAAAAJ">
                            Phong Tran<sup>2</sup>
                        </a>
                    </li>
                    <li>
                        <a href="https://sites.google.com/site/anhttranusc/">
                          Anh Tran<sup>1</sup>
                        </a>
                    </li>
                    <li>
                        <a href="https://sites.google.com/view/cuongpham/home">
                          Cuong Pham<sup>1,3</sup>
                        </a>
                    </li>
                    <li>
                        <a href="https://rangnguyen.github.io/">
                          Rang Nguyen<sup>1</sup>
                        </a>
                    </li>
                    <li>
                        <a href="https://www3.cs.stonybrook.edu/~minhhoai/">
                          Minh Hoai<sup>1,4</sup>
                        </a>
                    </li>
                    </br>
                    <!-- <br> -->
                    <li>
                        <sup>1</sup>VinAI Research
                    </li>
                    <li>
                        <sup>2</sup>MBZUAI
                    </li>
                    <li>
                        <sup>3</sup>PTIT
                    </li>
                    <li>
                        <sup>4</sup>Stony Brook University
                    </li>
                </ul>
            </div>
        </div>
        <div class="row">
            <h4 class="col-md-12 text-center">
                <!-- <small> -->
                    Our code would be released soon!
                <!-- </small> -->
            </h4>
        </div>
        <!-- <strong>Bang-Dang Pham</strong>,
              <a href="https://scholar.google.com/citations?hl=en&authuser=1&user=-BPaFHcAAAAJ">Phong Tran</a>,
              <a href="https://sites.google.com/site/anhttranusc/">Anh Tran</a>,
              <a href="https://sites.google.com/view/cuongpham/home">Cuong Pham</a>,
              <a href="https://rangnguyen.github.io/">Rang Nguyen</a>,
              <a href="https://www3.cs.stonybrook.edu/~minhhoai/">Minh Hoai</a> -->


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <!-- <a href="#">
                            <!-- <a href="https://arxiv.org/abs/2304.06706"> -->
                            <!-- "embed pdf here" -->
                                <!-- <embed src="img/Blur2Blur_Paper.pdf" type="application/pdf" width="10" height="10">
                                <image src="img/zip_paper_image.jpg" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a> -->
                            <!-- <a href="img/Blur2Blur_Paper.pdf" target="pdfViewer"> -->
                            <a href="img/Blur2Blur_Paper.pdf" download>
                                <img src="img/zip_blur2blur.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="/blur2blur">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                    </ul>
                    <!-- <iframe id="pdfViewer" name="pdfViewer" style="display: none;"></iframe> -->
                </div>
        </div>



        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/teaser.mp4" type="video/mp4" />
                </video>
						</div>
        </div> -->
        <br>
        <div class="row">
            <!-- <div class="col-md-8 col-md-offset-2"> -->
            <div class="col-md-8 col-md-offset-2 text-center">
                <!-- <h3>
                    Results
                </h3> -->
                <!-- <video class="video" id="xyalias" loop playsinline autoplay muted src="img/output_1440_2vids_rsblur.mp4" onplay="resizeAndPlay(this)"></video>
                <canvas class="videoMerge" id="xyaliasMerge"></canvas> -->
                <!-- <div class="video-compare-container" id="materialsDiv"> -->
                <video class="video" id="results" loop playsinline autoPlay muted src="img/output_1440_2vids_rsblur.mp4" onplay="resizeAndPlay(this)"></video>
                <canvas height=0 class="videoMerge" id="resultsMerge"></canvas>
                <!-- </div> -->
                <p class="text-center">
                   Deblurred result with real-blur captured from mobile device.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    This paper presents an innovative framework designed to train an image deblurring algorithm tailored to a specific camera device. This algorithm works by transforming a blurry input image, which is challenging to deblur, into another blurry image that is more amenable to deblurring. The transformation process, from one blurry state to another, leverages unpaired data consisting of sharp and blurry images captured by the target camera device. Learning this blur-to-blur transformation is inherently simpler than direct blur-to-sharp conversion, as it primarily involves modifying blur patterns rather than the intricate task of reconstructing fine image details. The efficacy of the proposed approach has been demonstrated through comprehensive experiments on various benchmarks, where it significantly outperforms state-of-the-art methods both quantitatively and qualitatively.
                </p>
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Overview
                </h3>
                <img src="img/Teaser.jpg" alt="Description of the image" width="100%">
                <br><br>
                <p class="text-justify">
                    We address the unsupervised image deblurring problem by training a blur translator that converts an input image with unknown blur to an image with a predefined known blur. The figure shows the effectiveness of our approach. The blurry images before and after translation (left image in each box) exhibit similar visual content but have different blur patterns (zoomed-in patches). While a standard image deblurring technique fails to restore the unknown-blur image, it successfully recovers the known-blur version, yielding an approximate 2.2 dB increase in PSNR score (noted below each deblurred image on the right side of each box
                </p>
            </div>
        </div>

        

<!-- 
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://youtube.com/embed/xrrhynRzC8k" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>
<br>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    360Â° Video Flythroughs
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://www.youtube.com/embed/videoseries?list=PLzPoYEE6Aw7Jzjek1uEIPnpcTDL3u8tb4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                        <!-- <iframe src="https://youtube.com/embed/jbE2ri8xEZo" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe> -->
                    <!-- </div>
                </div>
            </div>
        </div>
<br> --> 
        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Multisampling
                </h3>
				<table style="width: 100%; border-collapse: collapse;">
				  <tr>
				    <td style="text-align: center;">
		                <video id="v0" width="100%" autoplay loop muted>
		                  <source src="img/hexify_train.mp4" type="video/mp4" />
		                </video>
					</td>
				    <td style="text-align: center;">
		                <video id="v0" width="100%" autoplay loop muted>
		                  <source src="img/hexify_test.mp4" type="video/mp4" />
		                </video>
					</td>
				  </tr>
				  <tr>
				    <td style="text-align: center;">When Training</td>
				    <td style="text-align: center;">When Rendering</td>
				  </tr>
				</table>
                <p class="text-justify">
                    We use multisampling to approximate the average NGP feature over a conical frustum, by constructing a 6-sample pattern that exactly matches the frustum's first and second moments. When training, we randomly rotate and flip (along the ray axis) each pattern, and when rendering we deterministically flip and rotate each adjacent pattern by 30 degrees.
                </p>
            </div>
        </div> -->
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>Method</h3>
                <div style="text-align: center;">
                    <img src="img/Overview.jpg"  style="width: 90%;"">
                </div>
                <br>
                <p class="text-justify">
                    Given a camera, we aim to develop an algorithm to deblur its captured blurry images. We assume access to the camera to collect unpaired sets of blurry images (B) and sharp image sequences (S).
                </p>
                <br>
                <div style="text-align: center;">
                    <img src="img/B2B_Pipeline.jpg"  style="width: 90%;"">
                </div>
                <br>
                <p class="text-justify">
                    The key component in our proposed system is a blur translator that converts unknown-blur images captured by the camera to have the target
                    known-blur presented in K. This translator is trained using reconstruction and adversarial losses. The converted images have known blur and
                    can be successfully deblurred using the previously trained deblurring model (Zoom for best view).
                </p>
            </div>
        </div>
        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>Multisampling</h3>
                <table style="width: 100%; ">
                    <tr>
                        <td style="text-align: center;">
                            <img src="img/Overview.jpg" alt="Hexify Train" style="width: 100%; height: 100%;">
                        </td>
                        <td style="text-align: center;">
                            <img src="img/B2B_Pipeline.jpg" alt="Hexify Test" width="100%">
                        </td>
                    </tr>
                    <tr>
                        <td style="text-align: center;">When Training</td>
                        <td style="text-align: center;">When Rendering</td>
                    </tr>
                </table>
                <p class="text-justify">
                    We use multisampling to approximate the average NGP feature over a conical frustum, by constructing a 6-sample pattern that exactly matches the frustum's first and second moments. When training, we randomly rotate and flip (along the ray axis) each pattern, and when rendering we deterministically flip and rotate each adjacent pattern by 30 degrees.
                </p>
            </div>
        </div> -->
        
<br>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results
                </h3>
                <video id="v0" width="100%" autoplay loop muted controls>
                <source src="img/video1.mp4" type="video/mp4" />
                </video>
                <p class="text-justify">
                    To evaluate our Blur2Blur model for practical application, we used a video with pronounced hand movements, pre-training the deblurring model on the RSBlur dataset. 
                    The results clearly show that our Blur2Blur framework significantly enhances visual clarity compared 
                    to using the pre-trained deblurring model alone. Moreover, to further assess the enhancement in hand movement recognition, 
                    we validated the deblurred videos using the Hand Pose Estimation model from <a href="https://developers.google.com/mediapipe">MediaPipe</a>.

                    The results, shown in the video, highlight a notable improvement in hand pose estimation when using our method. 
                    The enhanced sharpness and detail achieved by Blur2Blur enable more accurate and reliable recognition of hand poses. 
                    This demonstrates the potential of our Blur2Blur model in applications demanding high-fidelity visualization of hand movements, 
                    especially in advanced rehabilitation therapy tools that rely on precise hand movement tracking for effective patient care and recovery.
                </p>
            </div>
        </div>

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    XY aliasing
                </h3>
                <video class="video" width=100% id="xyalias" loop playsinline autoplay muted src="img/xy_alias_swipe_crf27.mp4" onplay="resizeAndPlay(this)"></video>
                <canvas height=0 class="videoMerge" id="xyaliasMerge"></canvas>
                <p class="text-justify">
                    A naive baseline (left) combining mip-NeRF 360 and Instant NGP results in aliasing as the camera moves laterally. Our full method (right) produces prefiltered renderings that do not flicker or shimmer.
                </p>
            </div>
        </div>
           
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Z aliasing
                </h3>
                <video id="v0" width="100%" autoplay loop muted>
                  <source src="img/z_alias_pdf_labeled.m4v" type="video/mp4" />
                </video>
                <p class="text-justify">
                    The proposal network used for resampling points along rays in mip-NeRF 360 results in an artifact we refer to as <em>z-aliasing</em>, where foreground content alternately appears and disappears as the camera moves toward or away from scene content. Z-aliasing occurs when the initial set of samples from the proposal network is not dense enough and misses thin structures, such as the chair above. Missed content can not be recovered by later rounds of sampling, since no future samples will be placed at that location along the ray. Our improvements to proposal network supervision result in a prefiltered proposal output that preserves the foreground object for all frames in this sequence. The plots above depict samples along a ray for three rounds of resampling (blue, orange, and green lines), with the y axis showing rendering weight (how much each interval contributes to the final rendered color), as a normalized probability density.
                </p>
            </div>
        </div> -->

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@article{barron2023zipnerf,
    title={Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields},
    author={Jonathan T. Barron and Ben Mildenhall and 
            Dor Verbin and Pratul P. Srinivasan and Peter Hedman},
    journal={ICCV},
    year={2023}
}</textarea>
                </div>
            </div>
        </div> -->
        <center>
            <div style="display:none; color:#dedede; font-size:.5em;">
                <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=X6Yamx8jfOQCcH8DU6zsTpEnR8W9AMABmwewoSjJYVc"></script>
            </div>
        </center>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    <!-- <br><br> -->
                The website template was borrowed from <a href="http://mgharbi.com/">MichaÃ«l Gharbi</a> and <a href="https://dorverbin.github.io/refnerf">Ref-NeRF</a>.
                </p>
            </div>
        </div>

    </div>
</body>
</html>
<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=X6Yamx8jfOQCcH8DU6zsTpEnR8W9AMABmwewoSjJYVc"></script>